# BRIX-OCR

OCR engines have been developed into many kinds of domain-specific OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR. They can be used for: Data entry for business documents, e.g. Cheque, passport, invoice, bank statement and receipt. Automatic number plate recognition.
This project focuses to build a OCR engine with the help of datasets and images.

# Available libraries
Some of the avalable OCR engines are:
- tesseract-ocr By google, [GITHUB REPO](https://github.com/tesseract-ocr/tesseract) | [Documentation](https://tesseract-ocr.github.io/)
- keras-ocr, [GITHUB REPO](https://github.com/faustomorales/keras-ocr) | [Documentation](https://keras-ocr.readthedocs.io/en/latest/)
- EasyOCR by [Jaided AI](https://github.com/JaidedAI) , [GITHUB REPO](https://github.com/JaidedAI/EasyOCR) | [Documentation](https://www.jaided.ai/easyocr/)
- Text detection with MSER and SWT by @azmiozgen, [GITHUB REPO](https://github.com/azmiozgen/text-detection)
- TeOCR by Hugging Face , [Overview](https://huggingface.co/docs/transformers/model_doc/trocr)
- docTR by mindee, [GITHUB REPO](https://github.com/mindee/doctr) | [Documentation](https://mindee.github.io/doctr/)


# Datasets:
Some of the available datasets for testing and training a OCR engine:
- [list open dataset about ocr.](https://pythonrepo.com/repo/xylcbd-ocr-open-dataset-python-computer-vision)
- [Keras OCR datatset](https://thor.robots.ox.ac.uk/~vgg/data/text/mjsynth.tar.gz)
- [IAM Handwriting](https://paperswithcode.com/dataset/iam)
- [ICDAR 2003](https://paperswithcode.com/dataset/icdar-2003)
- [TextOCR](https://paperswithcode.com/dataset/textocr)
- [FUNSD (Form Understanding in Noisy Scanned Documents)](https://paperswithcode.com/dataset/funsd)
- [ST-VQA](https://paperswithcode.com/dataset/st-vqa)
- [SciTSR](https://paperswithcode.com/dataset/scitsr)
- [TextCaps](https://paperswithcode.com/dataset/textcaps)
- [DocBank](https://paperswithcode.com/dataset/docbank)
- [Kannada-MNIST](https://paperswithcode.com/dataset/kannada-mnist)
- [MLe2e](https://paperswithcode.com/dataset/mle2e)

# Research papers:
- [Attention-based Extraction of Structured Information from Street View Imagery](https://paperswithcode.com/paper/attention-based-extraction-of-structured)
- [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://paperswithcode.com/paper/trocr-transformer-based-optical-character)
- [Adapting the Tesseract Open Source OCR Engine for Multilingual OCR](https://paperswithcode.com/paper/adapting-the-tesseract-open-source-ocr-engine)
- [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight)
- [LayoutReader: Pre-training of Text and Layout for Reading Order Detection](https://paperswithcode.com/paper/layoutreader-pre-training-of-text-and-layout)
- [PP-OCR: A Practical Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr)
- [Robustness Evaluation of Transformer-based Form Field Extractors via Form Attacks](https://paperswithcode.com/paper/robustness-evaluation-of-transformer-based)
- [End-to-End Interpretation of the French Street Name Signs Dataset](https://paperswithcode.com/paper/end-to-end-interpretation-of-the-french)
- [MMOCR: A Comprehensive Toolbox for Text Detection, Recognition and Understanding](https://paperswithcode.com/paper/mmocr-a-comprehensive-toolbox-for-text)

# TASKS to resolve:
- **Task 0**: As we need to train the custom model as well as the pretrained models so we need datasets,please add datasets links or download them inside a drive and make hyper link in the [datasets sections](https://github.com/Artificial-Brix/BRIX-OCR#datasets) in the readme.md and complete the Task 0.
- **Task 1**: There are three folders given [Newspaper](https://github.com/Artificial-Brix/BRIX-OCR/tree/main/Newspaper) ,[Posters](https://github.com/Artificial-Brix/BRIX-OCR/tree/main/Posters) and [Sheets](https://github.com/Artificial-Brix/BRIX-OCR/tree/main/Sheets),go inside one folder, you can find a image there, as a sample, please find similar images only and push them inside perticuler folders, minimum 50 images inside a folder will be enough to make the dataset.
- **Task 2**: In this task you have to make a jupyter notebook and in that try to use some of the given [libraries](https://github.com/Artificial-Brix/BRIX-OCR#available-libraries) in the readme section and you have to test their output using the images in the [test images](https://github.com/Artificial-Brix/BRIX-OCR/tree/main/test_images),and contribute a jupyter notebook as a name like this: Name_of_the_contributer.ipynb.
- **Task 3**: This is the last step of the project, as you have tried all the libraries,make a custom model using the [datasets](https://github.com/Artificial-Brix/BRIX-OCR#datasets) and the take the help of the [research papers](https://github.com/Artificial-Brix/BRIX-OCR#research-papers) as well as you mentor of the project,make a jupyter notebook and complete the Task 3.



# How to contribute




